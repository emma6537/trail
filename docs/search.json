[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Self-Introduction\n\n\nBofang Jiang, second year graduate student in the master of city planning program.\nEmmanuel Jiang, graduate student in the urban spatial analytics program.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Analysis",
      "Python code blocks"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot.",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThe purpose of the project is to use the Utah Real Estate dataset to predict real estate listing prices through visual clustering algorithms and machine learning regression models. The dataset contains 4,440 entries and 14 attributes that comprehensively characterize property features and conditions. The analysis will explore the relationships between variables and assess the impact of different attributes on listing prices to identify key determinants of property values. The ultimate goal is to identify the most accurate model for predicting home prices, providing insights into the real estate market and a valuable reference for property valuation and investment decisions. This study helps to improve the understanding of the real estate market, optimize investment decisions, and promote the healthy development of the real estate market.\nThe following begins to describe the analysis of this study.",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research on listing price prediction of real estate based on machine learning model",
    "section": "",
    "text": "We are interested in home price prediction and have selected a Utah real estate dataset for our project analysis.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550: Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/Qmdpython.html",
    "href": "analysis/Qmdpython.html",
    "title": "Loading libraries",
    "section": "",
    "text": "# Import the packages we'll need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.offline as py\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nimport matplotlib.ticker as mtick\nfrom yellowbrick.cluster import KElbowVisualizer  \nfrom sklearn.linear_model import BayesianRidge, LinearRegression, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score    \nfrom sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = 999\nLoad the libraries required in this study, mainly data processing libraries, machine learning algorithm libraries, clustering algorithm libraries and interactive visualization libraries."
  },
  {
    "objectID": "analysis/Qmdpython.html#descriptive-analysis",
    "href": "analysis/Qmdpython.html#descriptive-analysis",
    "title": "Loading libraries",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\n\nCode\nfrom itables import init_notebook_mode, show\ndf = pd.read_csv(\"real_estate_utah.csv\")\ninit_notebook_mode(all_interactive=False)\nshow(df)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      type\n      text\n      year_built\n      beds\n      baths\n      baths_full\n      baths_half\n      garage\n      lot_sqft\n      sqft\n      stories\n      lastSoldOn\n      listPrice\n      status\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nDemonstrate the overall value of the dataset\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4440 entries, 0 to 4439\nData columns (total 14 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   type        4440 non-null   object \n 1   text        4440 non-null   object \n 2   year_built  4440 non-null   float64\n 3   beds        4440 non-null   float64\n 4   baths       4440 non-null   float64\n 5   baths_full  4440 non-null   float64\n 6   baths_half  4440 non-null   float64\n 7   garage      4440 non-null   float64\n 8   lot_sqft    4440 non-null   float64\n 9   sqft        4440 non-null   float64\n 10  stories     4440 non-null   float64\n 11  lastSoldOn  4440 non-null   object \n 12  listPrice   4440 non-null   float64\n 13  status      4440 non-null   object \ndtypes: float64(10), object(4)\nmemory usage: 485.8+ KB\n\n\nOutputs a brief summary showing information about the dataframe, including the index’s datatype dtype and the column’s datatype dtype, the number of non-null values, and memory usage.\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nbeds\n4421.0\n3.898665\n1.0\n3.0\n4.0\n4.0\n19.0\n1.266574\n\n\nbaths\n4421.0\n2.452839\n0.0\n2.0\n3.0\n3.0\n45.0\n1.792608\n\n\nbaths_full\n4421.0\n2.239086\n1.0\n2.0\n2.0\n3.0\n45.0\n1.167265\n\n\nbaths_half\n4421.0\n1.025107\n1.0\n1.0\n1.0\n1.0\n6.0\n0.195083\n\n\ngarage\n4421.0\n2.335897\n0.0\n2.0\n2.0\n2.0\n20.0\n1.026591\n\n\nlot_sqft\n4421.0\n554838.609138\n436.0\n9583.0\n13939.0\n24829.0\n600953760.0\n11369016.504254\n\n\nstories\n4421.0\n2.000679\n1.0\n2.0\n2.0\n2.0\n4.0\n0.629407\n\n\nlastSoldOn\n4421\n2017-06-28 00:07:09.947975680\n1995-08-15 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2024-01-30 00:00:00\nNaN\n\n\nage\n4421.0\n27.078941\n-1.0\n18.0\n22.0\n28.0\n165.0\n23.657422\n\n\npricePerSqft\n4421.0\n272.359367\n0.625\n167.225951\n224.675886\n294.125\n8312.5\n380.219469\n\n\n\n\n\n\n\ncount: the number of non-null values in each column\nmean: the mean of each column\nstd: standard deviation of each column\nmin: minimum value\n25%: 25% quantile, after sorting the number in the 25% position\n50%: 50% quartile\n75%: 75% quartile\nmax:maximum value\n\n\nCode\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #Visual inspection of missing values\n\n\n\n\n\n\n\n\n\nChecking for missing data revealed no missing values for the variables in the dataset.\n\n\nCode\ntype_mapping = {\n    'single_family': 'single_family',\n    'land': 'land',\n    'townhomes': 'townhome',\n    'townhouse': 'townhome',\n    'mobile': 'mobile',\n    'condos': 'condo',\n    'condo_townhome_rowhome_coop': 'condo',\n    'condo_townhome': 'condo',\n    'condo': 'condo',\n    'other': 'other',\n    'farm': 'farm'\n}\n\n\n\n\nCode\ndf['type'] = df['type'].map(type_mapping)\ndf['year_built'] = df['year_built'].astype('int')\ndf['age'] = pd.to_datetime('today').year - df['year_built']\ndf= df.loc[(df[\"sqft\"] &gt; 0.0) & (df['listPrice'] &gt; 0.0)]\ndf['pricePerSqft'] = df['listPrice'] / df['sqft'] #create feature\ndf.drop('year_built', axis=1, inplace=True)\ndf['lastSoldOn'] = pd.to_datetime(df['lastSoldOn'])\ndf = df.drop(['listPrice','sqft'],axis=1)"
  },
  {
    "objectID": "analysis/Qmdpython.html#interactive-visual-analytics",
    "href": "analysis/Qmdpython.html#interactive-visual-analytics",
    "title": "Loading libraries",
    "section": "Interactive Visual Analytics",
    "text": "Interactive Visual Analytics\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart."
  },
  {
    "objectID": "analysis/Introduction.html",
    "href": "analysis/Introduction.html",
    "title": "Data Preparation",
    "section": "",
    "text": "# Import the packages we'll need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.offline as py\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nimport matplotlib.ticker as mtick\nfrom yellowbrick.cluster import KElbowVisualizer  \nfrom sklearn.linear_model import BayesianRidge, LinearRegression, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score    \nfrom sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = 999\n\nLoad the libraries required in this study, mainly data processing libraries, machine learning algorithm libraries, clustering algorithm libraries and interactive visualization libraries.",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/Introduction.html#descriptive-analysis",
    "href": "analysis/Introduction.html#descriptive-analysis",
    "title": "Data Preparation",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4440 entries, 0 to 4439\nData columns (total 14 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   type        4440 non-null   object \n 1   text        4440 non-null   object \n 2   year_built  4440 non-null   float64\n 3   beds        4440 non-null   float64\n 4   baths       4440 non-null   float64\n 5   baths_full  4440 non-null   float64\n 6   baths_half  4440 non-null   float64\n 7   garage      4440 non-null   float64\n 8   lot_sqft    4440 non-null   float64\n 9   sqft        4440 non-null   float64\n 10  stories     4440 non-null   float64\n 11  lastSoldOn  4440 non-null   object \n 12  listPrice   4440 non-null   float64\n 13  status      4440 non-null   object \ndtypes: float64(10), object(4)\nmemory usage: 485.8+ KB\n\n\nOutputs a brief summary showing information about the dataframe, including the index’s datatype dtype and the column’s datatype dtype, the number of non-null values, and memory usage.\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nbeds\n4421.0\n3.898665\n1.0\n3.0\n4.0\n4.0\n19.0\n1.266574\n\n\nbaths\n4421.0\n2.452839\n0.0\n2.0\n3.0\n3.0\n45.0\n1.792608\n\n\nbaths_full\n4421.0\n2.239086\n1.0\n2.0\n2.0\n3.0\n45.0\n1.167265\n\n\nbaths_half\n4421.0\n1.025107\n1.0\n1.0\n1.0\n1.0\n6.0\n0.195083\n\n\ngarage\n4421.0\n2.335897\n0.0\n2.0\n2.0\n2.0\n20.0\n1.026591\n\n\nlot_sqft\n4421.0\n554838.609138\n436.0\n9583.0\n13939.0\n24829.0\n600953760.0\n11369016.504254\n\n\nstories\n4421.0\n2.000679\n1.0\n2.0\n2.0\n2.0\n4.0\n0.629407\n\n\nlastSoldOn\n4421\n2017-06-28 00:07:09.947975680\n1995-08-15 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2024-01-30 00:00:00\nNaN\n\n\nage\n4421.0\n27.078941\n-1.0\n18.0\n22.0\n28.0\n165.0\n23.657422\n\n\npricePerSqft\n4421.0\n272.359367\n0.625\n167.225951\n224.675886\n294.125\n8312.5\n380.219469\n\n\n\n\n\n\n\ncount: the number of non-null values in each column\nmean: the mean of each column\nstd: standard deviation of each column\nmin: minimum value\n25%: 25% quantile, after sorting the number in the 25% position\n50%: 50% quartile\n75%: 75% quartile\nmax:maximum value",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/Introduction.html#interactive-visual-analytics",
    "href": "analysis/Introduction.html#interactive-visual-analytics",
    "title": "Loading libraries",
    "section": "Interactive Visual Analytics",
    "text": "Interactive Visual Analytics\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart.",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Machine Learning.html",
    "href": "analysis/Machine Learning.html",
    "title": "Machine Learning Algorithms",
    "section": "",
    "text": "Five machine learning models were constructed to predict home prices in Utah: Bayesian Ridge, Linear Regression, ElasticNet, SVR, and GBR. These models are trained based on the characteristics of houses (e.g., size, number of bedrooms, etc.) to predict house prices. To evaluate the performance of the models, a cross-validation approach was used to ensure the stability and reliability of the results through multiple training and testing.\n\n\nCode\nX = df.drop(['text','pricePerSqft',\"lastSoldOn\"],axis=1)\ny = df['pricePerSqft']\nX = pd.get_dummies(X, drop_first=True, dtype='int')\n\n\n\n\nCode\nX = X\ny = df['pricePerSqft'].values\nn_folds=5\n#Bayesling regression model\nbr_model = BayesianRidge()\n#Linear regression\nlr_model = LinearRegression()\n#Elastic network regression model\netc_model = ElasticNet()\n#Support vector machine regression\nsvr_model = SVR()\n#Gradient enhanced regression model\ngbr_model = GradientBoostingRegressor(random_state=1)\n#A list of names for different models\nmodel_names = ['BayesianRidge', 'LinearRegression', 'ElasticNet', 'SVR', 'GBR']\nmodel_dic = [br_model, lr_model, etc_model, svr_model, gbr_model]\ncv_score_list = []\npre_y_list = []\nfor model in model_dic:\n    scores = cross_val_score(model,X,y,cv=n_folds) #Model cross-validation\n    cv_score_list.append(scores)                  \n    pre_y_list.append(model.fit(X,y).predict(X))   #Prediction results on the test set\ndf_score = pd.DataFrame(cv_score_list, index=model_names)\ndf_score\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nBayesianRidge\n0.331879\n0.476998\n-0.246165\n0.375415\n0.245122\n\n\nLinearRegression\n0.332096\n0.483609\n-0.253943\n0.378228\n0.245462\n\n\nElasticNet\n0.130439\n0.001475\n-0.010112\n0.047115\n0.087906\n\n\nSVR\n-0.026903\n-0.018592\n-0.007262\n-0.012425\n-0.021750\n\n\nGBR\n0.591863\n0.481195\n0.190811\n0.603570\n0.266486\n\n\n\n\n\n\n\n\n\nCode\n### Evaluation of model effect index ###\nn_sample, n_feature = X.shape\n# Regression evaluation index object list\nmodel_metrics_name = [explained_variance_score,\n                      mean_absolute_error,\n                      mean_squared_error,r2_score]\n# Regression evaluation index list\nmodel_metrics_list = []\n# Loop through the predictions of each model\nfor pre_y in pre_y_list:\n    tmp_list = []\n    for mdl in model_metrics_name:\n        tmp_score = mdl(y, pre_y)\n        tmp_list.append(tmp_score)\n    model_metrics_list.append(tmp_list)\ndf_met = pd.DataFrame(model_metrics_list, index=model_names,\n                      columns=['ev', 'mae', 'mse', 'r2'])\ndf_met\n\n\n\n\n\n\n\n\n\nev\nmae\nmse\nr2\n\n\n\n\nBayesianRidge\n0.310648\n116.273722\n99634.907167\n0.310648\n\n\nLinearRegression\n0.310672\n116.214159\n99631.443272\n0.310672\n\n\nElasticNet\n0.038671\n125.347216\n138944.822627\n0.038671\n\n\nSVR\n0.004330\n121.447129\n146445.779432\n-0.013226\n\n\nGBR\n0.728965\n88.184705\n39173.747357\n0.728965\n\n\n\n\n\n\n\nBased on the evaluation metrics provided, the gradient boosting regression model demonstrated optimal performance in predicting Utah home prices. The mean absolute error and mean square error were better than other models, and the R-squared value was as high as 0.728965, indicating that the model explains a large portion of the home price variability. Despite the relatively low mae of ElasticNet, GBR is superior in terms of the composite metrics, especially its mse is much lower than that of the other models, showing higher prediction accuracy. Thus, the gradient boosted regression model is considered the optimal choice for predicting house prices in Utah.\n\n\nCode\nplt.figure(figsize=(9, 7))\ncolor_list = ['r','g','b','y','c']\nfor i, pre_y in enumerate(pre_y_list):\n    plt.subplot(2, 3, i+1)\n    plt.plot(np.arange(X.shape[0]), y, color='k', label='y')\n    # Draw prediction lines for each model\n    plt.plot(np.arange(X.shape[0]),\n             pre_y,\n             color_list[i],\n             label=model_names[i])\n    plt.title(model_names[i])\n    plt.legend(loc='lower left')\nplt.show()\n\n\n\n\n\n\n\n\n\nBy comparing the results of multiple regression models, we can see that the best model is the gradient enhanced regression model.",
    "crumbs": [
      "Analysis",
      "Machine Learning Algorithms"
    ]
  },
  {
    "objectID": "analysis/Machine Learning.html#descriptive-analysis",
    "href": "analysis/Machine Learning.html#descriptive-analysis",
    "title": "Loading libraries",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\n\nCode\nfrom itables import init_notebook_mode, show\ndf = pd.read_csv(\"real_estate_utah.csv\")\ninit_notebook_mode(all_interactive=False)\nshow(df)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      type\n      text\n      year_built\n      beds\n      baths\n      baths_full\n      baths_half\n      garage\n      lot_sqft\n      sqft\n      stories\n      lastSoldOn\n      listPrice\n      status\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nDemonstrate the overall value of the dataset\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4440 entries, 0 to 4439\nData columns (total 14 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   type        4440 non-null   object \n 1   text        4440 non-null   object \n 2   year_built  4440 non-null   float64\n 3   beds        4440 non-null   float64\n 4   baths       4440 non-null   float64\n 5   baths_full  4440 non-null   float64\n 6   baths_half  4440 non-null   float64\n 7   garage      4440 non-null   float64\n 8   lot_sqft    4440 non-null   float64\n 9   sqft        4440 non-null   float64\n 10  stories     4440 non-null   float64\n 11  lastSoldOn  4440 non-null   object \n 12  listPrice   4440 non-null   float64\n 13  status      4440 non-null   object \ndtypes: float64(10), object(4)\nmemory usage: 485.8+ KB\n\n\nOutputs a brief summary showing information about the dataframe, including the index’s datatype dtype and the column’s datatype dtype, the number of non-null values, and memory usage.\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nbeds\n4421.0\n3.898665\n1.0\n3.0\n4.0\n4.0\n19.0\n1.266574\n\n\nbaths\n4421.0\n2.452839\n0.0\n2.0\n3.0\n3.0\n45.0\n1.792608\n\n\nbaths_full\n4421.0\n2.239086\n1.0\n2.0\n2.0\n3.0\n45.0\n1.167265\n\n\nbaths_half\n4421.0\n1.025107\n1.0\n1.0\n1.0\n1.0\n6.0\n0.195083\n\n\ngarage\n4421.0\n2.335897\n0.0\n2.0\n2.0\n2.0\n20.0\n1.026591\n\n\nlot_sqft\n4421.0\n554838.609138\n436.0\n9583.0\n13939.0\n24829.0\n600953760.0\n11369016.504254\n\n\nstories\n4421.0\n2.000679\n1.0\n2.0\n2.0\n2.0\n4.0\n0.629407\n\n\nlastSoldOn\n4421\n2017-06-28 00:07:09.947975680\n1995-08-15 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2024-01-30 00:00:00\nNaN\n\n\nage\n4421.0\n27.078941\n-1.0\n18.0\n22.0\n28.0\n165.0\n23.657422\n\n\npricePerSqft\n4421.0\n272.359367\n0.625\n167.225951\n224.675886\n294.125\n8312.5\n380.219469\n\n\n\n\n\n\n\ncount: the number of non-null values in each column\nmean: the mean of each column\nstd: standard deviation of each column\nmin: minimum value\n25%: 25% quantile, after sorting the number in the 25% position\n50%: 50% quartile\n75%: 75% quartile\nmax:maximum value\n\n\nCode\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #Visual inspection of missing values\n\n\n\n\n\n\n\n\n\nChecking for missing data revealed no missing values for the variables in the dataset.\n\n\nCode\ntype_mapping = {\n    'single_family': 'single_family',\n    'land': 'land',\n    'townhomes': 'townhome',\n    'townhouse': 'townhome',\n    'mobile': 'mobile',\n    'condos': 'condo',\n    'condo_townhome_rowhome_coop': 'condo',\n    'condo_townhome': 'condo',\n    'condo': 'condo',\n    'other': 'other',\n    'farm': 'farm'\n}\n\n\n\n\nCode\ndf['type'] = df['type'].map(type_mapping)\ndf['year_built'] = df['year_built'].astype('int')\ndf['age'] = pd.to_datetime('today').year - df['year_built']\ndf= df.loc[(df[\"sqft\"] &gt; 0.0) & (df['listPrice'] &gt; 0.0)]\ndf['pricePerSqft'] = df['listPrice'] / df['sqft'] #create feature\ndf.drop('year_built', axis=1, inplace=True)\ndf['lastSoldOn'] = pd.to_datetime(df['lastSoldOn'])\ndf = df.drop(['listPrice','sqft'],axis=1)",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Machine Learning.html#interactive-visual-analytics",
    "href": "analysis/Machine Learning.html#interactive-visual-analytics",
    "title": "Loading libraries",
    "section": "Interactive Visual Analytics",
    "text": "Interactive Visual Analytics\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart.",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Clustering Algorithms.html",
    "href": "analysis/Clustering Algorithms.html",
    "title": "Cluster analysis",
    "section": "",
    "text": "Code\ndf[\"lot_sqft\"] = (df[\"lot_sqft\"] - df[\"lot_sqft\"].min()) / (df[\"lot_sqft\"].max() - df[\"lot_sqft\"].min())\ndfclu = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\",\"age\"]]\n\n\n\n\nCode\nK = range(1, 10)\nmeandistortions = []\nfor k in K:\n    kmeans = KMeans(n_clusters=k,random_state=1)\n    kmeans.fit(dfclu)\n    meandistortions.append(sum(np.min(cdist(dfclu , kmeans.cluster_centers_, 'euclidean'), axis=1))/df1.shape[0])\nplt.plot(K, meandistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average Dispersion')\nplt.title('Selecting k with the Elbow Method')\nplt.gca().yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe optimal number of clusters for the K-means clustering algorithm is selected by the elbow method, k. First, a range of cluster numbers from 1 to 9 is defined, and then for each value of k, the mean aberration after clustering, i.e., the average distance of a sample to its nearest cluster center, is calculated. Finally, the k values are plotted against the mean aberration to aid in the selection of the optimal k value.\n\n\nCode\nmodel = KMeans(random_state=1)  \nvisualizer = KElbowVisualizer(\n    model, \n    k=(2,10),\n    timings=True,  # \n    metric=\"distortion\",  # distortion, silhouette,calinski_harabasz\n    )\nvisualizer.fit(dfclu)        \nvisualizer.show()  \n\n\n\n\n\n\n\n\n\nThe KMeans clustering model and the KElbowVisualizer visualization tool are used to select the optimal number of clusters, and the KElbowVisualizer helps the user find the “elbow” intuitively by plotting the distortion at different k-values, i.e., the point of k-value where the distortion starts to decrease sharply and becomes slower, and the optimal number of clusters is obtained. values, and get the optimal number of clusters4.",
    "crumbs": [
      "Analysis",
      "Cluster analysis"
    ]
  },
  {
    "objectID": "analysis/Clustering Algorithms.html#descriptive-analysis",
    "href": "analysis/Clustering Algorithms.html#descriptive-analysis",
    "title": "Loading libraries",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\n\nCode\nfrom itables import init_notebook_mode, show\ndf = pd.read_csv(\"real_estate_utah.csv\")\ninit_notebook_mode(all_interactive=False)\nshow(df)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      type\n      text\n      year_built\n      beds\n      baths\n      baths_full\n      baths_half\n      garage\n      lot_sqft\n      sqft\n      stories\n      lastSoldOn\n      listPrice\n      status\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nDemonstrate the overall value of the dataset\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4440 entries, 0 to 4439\nData columns (total 14 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   type        4440 non-null   object \n 1   text        4440 non-null   object \n 2   year_built  4440 non-null   float64\n 3   beds        4440 non-null   float64\n 4   baths       4440 non-null   float64\n 5   baths_full  4440 non-null   float64\n 6   baths_half  4440 non-null   float64\n 7   garage      4440 non-null   float64\n 8   lot_sqft    4440 non-null   float64\n 9   sqft        4440 non-null   float64\n 10  stories     4440 non-null   float64\n 11  lastSoldOn  4440 non-null   object \n 12  listPrice   4440 non-null   float64\n 13  status      4440 non-null   object \ndtypes: float64(10), object(4)\nmemory usage: 485.8+ KB\n\n\nOutputs a brief summary showing information about the dataframe, including the index’s datatype dtype and the column’s datatype dtype, the number of non-null values, and memory usage.\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nbeds\n4421.0\n3.898665\n1.0\n3.0\n4.0\n4.0\n19.0\n1.266574\n\n\nbaths\n4421.0\n2.452839\n0.0\n2.0\n3.0\n3.0\n45.0\n1.792608\n\n\nbaths_full\n4421.0\n2.239086\n1.0\n2.0\n2.0\n3.0\n45.0\n1.167265\n\n\nbaths_half\n4421.0\n1.025107\n1.0\n1.0\n1.0\n1.0\n6.0\n0.195083\n\n\ngarage\n4421.0\n2.335897\n0.0\n2.0\n2.0\n2.0\n20.0\n1.026591\n\n\nlot_sqft\n4421.0\n554838.609138\n436.0\n9583.0\n13939.0\n24829.0\n600953760.0\n11369016.504254\n\n\nstories\n4421.0\n2.000679\n1.0\n2.0\n2.0\n2.0\n4.0\n0.629407\n\n\nlastSoldOn\n4421\n2017-06-28 00:07:09.947975680\n1995-08-15 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2024-01-30 00:00:00\nNaN\n\n\nage\n4421.0\n27.078941\n-1.0\n18.0\n22.0\n28.0\n165.0\n23.657422\n\n\npricePerSqft\n4421.0\n272.359367\n0.625\n167.225951\n224.675886\n294.125\n8312.5\n380.219469\n\n\n\n\n\n\n\ncount: the number of non-null values in each column\nmean: the mean of each column\nstd: standard deviation of each column\nmin: minimum value\n25%: 25% quantile, after sorting the number in the 25% position\n50%: 50% quartile\n75%: 75% quartile\nmax:maximum value\n\n\nCode\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #Visual inspection of missing values\n\n\n\n\n\n\n\n\n\nChecking for missing data revealed no missing values for the variables in the dataset.\n\n\nCode\ntype_mapping = {\n    'single_family': 'single_family',\n    'land': 'land',\n    'townhomes': 'townhome',\n    'townhouse': 'townhome',\n    'mobile': 'mobile',\n    'condos': 'condo',\n    'condo_townhome_rowhome_coop': 'condo',\n    'condo_townhome': 'condo',\n    'condo': 'condo',\n    'other': 'other',\n    'farm': 'farm'\n}\n\n\n\n\nCode\ndf['type'] = df['type'].map(type_mapping)\ndf['year_built'] = df['year_built'].astype('int')\ndf['age'] = pd.to_datetime('today').year - df['year_built']\ndf= df.loc[(df[\"sqft\"] &gt; 0.0) & (df['listPrice'] &gt; 0.0)]\ndf['pricePerSqft'] = df['listPrice'] / df['sqft'] #create feature\ndf.drop('year_built', axis=1, inplace=True)\ndf['lastSoldOn'] = pd.to_datetime(df['lastSoldOn'])\ndf = df.drop(['listPrice','sqft'],axis=1)"
  },
  {
    "objectID": "analysis/Clustering Algorithms.html#interactive-visual-analytics",
    "href": "analysis/Clustering Algorithms.html#interactive-visual-analytics",
    "title": "Loading libraries",
    "section": "Interactive Visual Analytics",
    "text": "Interactive Visual Analytics\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart."
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html",
    "href": "analysis/Advanced Data Visualization.html",
    "title": "Advanced Visualization",
    "section": "",
    "text": "Code\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#descriptive-analysis",
    "href": "analysis/Advanced Data Visualization.html#descriptive-analysis",
    "title": "Loading libraries",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\n\nCode\nfrom itables import init_notebook_mode, show\ndf = pd.read_csv(\"real_estate_utah.csv\")\ninit_notebook_mode(all_interactive=False)\nshow(df)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      type\n      text\n      year_built\n      beds\n      baths\n      baths_full\n      baths_half\n      garage\n      lot_sqft\n      sqft\n      stories\n      lastSoldOn\n      listPrice\n      status\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nDemonstrate the overall value of the dataset\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4440 entries, 0 to 4439\nData columns (total 14 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   type        4440 non-null   object \n 1   text        4440 non-null   object \n 2   year_built  4440 non-null   float64\n 3   beds        4440 non-null   float64\n 4   baths       4440 non-null   float64\n 5   baths_full  4440 non-null   float64\n 6   baths_half  4440 non-null   float64\n 7   garage      4440 non-null   float64\n 8   lot_sqft    4440 non-null   float64\n 9   sqft        4440 non-null   float64\n 10  stories     4440 non-null   float64\n 11  lastSoldOn  4440 non-null   object \n 12  listPrice   4440 non-null   float64\n 13  status      4440 non-null   object \ndtypes: float64(10), object(4)\nmemory usage: 485.8+ KB\n\n\nOutputs a brief summary showing information about the dataframe, including the index’s datatype dtype and the column’s datatype dtype, the number of non-null values, and memory usage.\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nbeds\n4421.0\n3.898665\n1.0\n3.0\n4.0\n4.0\n19.0\n1.266574\n\n\nbaths\n4421.0\n2.452839\n0.0\n2.0\n3.0\n3.0\n45.0\n1.792608\n\n\nbaths_full\n4421.0\n2.239086\n1.0\n2.0\n2.0\n3.0\n45.0\n1.167265\n\n\nbaths_half\n4421.0\n1.025107\n1.0\n1.0\n1.0\n1.0\n6.0\n0.195083\n\n\ngarage\n4421.0\n2.335897\n0.0\n2.0\n2.0\n2.0\n20.0\n1.026591\n\n\nlot_sqft\n4421.0\n554838.609138\n436.0\n9583.0\n13939.0\n24829.0\n600953760.0\n11369016.504254\n\n\nstories\n4421.0\n2.000679\n1.0\n2.0\n2.0\n2.0\n4.0\n0.629407\n\n\nlastSoldOn\n4421\n2017-06-28 00:07:09.947975680\n1995-08-15 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2024-01-30 00:00:00\nNaN\n\n\nage\n4421.0\n27.078941\n-1.0\n18.0\n22.0\n28.0\n165.0\n23.657422\n\n\npricePerSqft\n4421.0\n272.359367\n0.625\n167.225951\n224.675886\n294.125\n8312.5\n380.219469\n\n\n\n\n\n\n\ncount: the number of non-null values in each column\nmean: the mean of each column\nstd: standard deviation of each column\nmin: minimum value\n25%: 25% quantile, after sorting the number in the 25% position\n50%: 50% quartile\n75%: 75% quartile\nmax:maximum value\n\n\nCode\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #Visual inspection of missing values\n\n\n\n\n\n\n\n\n\nChecking for missing data revealed no missing values for the variables in the dataset.\n\n\nCode\ntype_mapping = {\n    'single_family': 'single_family',\n    'land': 'land',\n    'townhomes': 'townhome',\n    'townhouse': 'townhome',\n    'mobile': 'mobile',\n    'condos': 'condo',\n    'condo_townhome_rowhome_coop': 'condo',\n    'condo_townhome': 'condo',\n    'condo': 'condo',\n    'other': 'other',\n    'farm': 'farm'\n}\n\n\n\n\nCode\ndf['type'] = df['type'].map(type_mapping)\ndf['year_built'] = df['year_built'].astype('int')\ndf['age'] = pd.to_datetime('today').year - df['year_built']\ndf= df.loc[(df[\"sqft\"] &gt; 0.0) & (df['listPrice'] &gt; 0.0)]\ndf['pricePerSqft'] = df['listPrice'] / df['sqft'] #create feature\ndf.drop('year_built', axis=1, inplace=True)\ndf['lastSoldOn'] = pd.to_datetime(df['lastSoldOn'])\ndf = df.drop(['listPrice','sqft'],axis=1)",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#interactive-visual-analytics",
    "href": "analysis/Advanced Data Visualization.html#interactive-visual-analytics",
    "title": "Loading libraries",
    "section": "Interactive Visual Analytics",
    "text": "Interactive Visual Analytics\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart.",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Conclusion.html",
    "href": "analysis/Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "This study analyzes Utah real estate data, visualizes the relationships between variables through data visualization techniques, and obtains the target variable of the study: unit home prices through feature engineering techniques. The optimal mean kmeans model was obtained using clustering techniques, and by constructing a total of five machine learning models, Bayesian Ridge (Bayesian Ridge Regression), Linear Regression, ElasticNet (Elastic Network Regression), SVR (Support Vector Regression), and GBR (Gradient Boosted Regression), using cross-validation and Grid tuning and other methods, to get the optimal algorithm for predicting house prices is the gradient boosting regression model, and based on the gradient boosting regression model to get the ranking of factors affecting house prices. The research has important value and significance.",
    "crumbs": [
      "Analysis",
      "Conclusion"
    ]
  },
  {
    "objectID": "analysis/Conclusion.html#descriptive-analysis",
    "href": "analysis/Conclusion.html#descriptive-analysis",
    "title": "Loading libraries",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\n\nCode\nfrom itables import init_notebook_mode, show\ndf = pd.read_csv(\"real_estate_utah.csv\")\ninit_notebook_mode(all_interactive=False)\nshow(df)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      type\n      text\n      year_built\n      beds\n      baths\n      baths_full\n      baths_half\n      garage\n      lot_sqft\n      sqft\n      stories\n      lastSoldOn\n      listPrice\n      status\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nDemonstrate the overall value of the dataset\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4440 entries, 0 to 4439\nData columns (total 14 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   type        4440 non-null   object \n 1   text        4440 non-null   object \n 2   year_built  4440 non-null   float64\n 3   beds        4440 non-null   float64\n 4   baths       4440 non-null   float64\n 5   baths_full  4440 non-null   float64\n 6   baths_half  4440 non-null   float64\n 7   garage      4440 non-null   float64\n 8   lot_sqft    4440 non-null   float64\n 9   sqft        4440 non-null   float64\n 10  stories     4440 non-null   float64\n 11  lastSoldOn  4440 non-null   object \n 12  listPrice   4440 non-null   float64\n 13  status      4440 non-null   object \ndtypes: float64(10), object(4)\nmemory usage: 485.8+ KB\n\n\nOutputs a brief summary showing information about the dataframe, including the index’s datatype dtype and the column’s datatype dtype, the number of non-null values, and memory usage.\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nbeds\n4421.0\n3.898665\n1.0\n3.0\n4.0\n4.0\n19.0\n1.266574\n\n\nbaths\n4421.0\n2.452839\n0.0\n2.0\n3.0\n3.0\n45.0\n1.792608\n\n\nbaths_full\n4421.0\n2.239086\n1.0\n2.0\n2.0\n3.0\n45.0\n1.167265\n\n\nbaths_half\n4421.0\n1.025107\n1.0\n1.0\n1.0\n1.0\n6.0\n0.195083\n\n\ngarage\n4421.0\n2.335897\n0.0\n2.0\n2.0\n2.0\n20.0\n1.026591\n\n\nlot_sqft\n4421.0\n554838.609138\n436.0\n9583.0\n13939.0\n24829.0\n600953760.0\n11369016.504254\n\n\nstories\n4421.0\n2.000679\n1.0\n2.0\n2.0\n2.0\n4.0\n0.629407\n\n\nlastSoldOn\n4421\n2017-06-28 00:07:09.947975680\n1995-08-15 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2018-05-31 00:00:00\n2024-01-30 00:00:00\nNaN\n\n\nage\n4421.0\n27.078941\n-1.0\n18.0\n22.0\n28.0\n165.0\n23.657422\n\n\npricePerSqft\n4421.0\n272.359367\n0.625\n167.225951\n224.675886\n294.125\n8312.5\n380.219469\n\n\n\n\n\n\n\ncount: the number of non-null values in each column\nmean: the mean of each column\nstd: standard deviation of each column\nmin: minimum value\n25%: 25% quantile, after sorting the number in the 25% position\n50%: 50% quartile\n75%: 75% quartile\nmax:maximum value\n\n\nCode\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #Visual inspection of missing values\n\n\n\n\n\n\n\n\n\nChecking for missing data revealed no missing values for the variables in the dataset.\n\n\nCode\ntype_mapping = {\n    'single_family': 'single_family',\n    'land': 'land',\n    'townhomes': 'townhome',\n    'townhouse': 'townhome',\n    'mobile': 'mobile',\n    'condos': 'condo',\n    'condo_townhome_rowhome_coop': 'condo',\n    'condo_townhome': 'condo',\n    'condo': 'condo',\n    'other': 'other',\n    'farm': 'farm'\n}\n\n\n\n\nCode\ndf['type'] = df['type'].map(type_mapping)\ndf['year_built'] = df['year_built'].astype('int')\ndf['age'] = pd.to_datetime('today').year - df['year_built']\ndf= df.loc[(df[\"sqft\"] &gt; 0.0) & (df['listPrice'] &gt; 0.0)]\ndf['pricePerSqft'] = df['listPrice'] / df['sqft'] #create feature\ndf.drop('year_built', axis=1, inplace=True)\ndf['lastSoldOn'] = pd.to_datetime(df['lastSoldOn'])\ndf = df.drop(['listPrice','sqft'],axis=1)",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Conclusion.html#interactive-visual-analytics",
    "href": "analysis/Conclusion.html#interactive-visual-analytics",
    "title": "Loading libraries",
    "section": "Interactive Visual Analytics",
    "text": "Interactive Visual Analytics\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart.",
    "crumbs": [
      "Analysis",
      "Loading libraries"
    ]
  },
  {
    "objectID": "analysis/Introduction.html#environment-setup",
    "href": "analysis/Introduction.html#environment-setup",
    "title": "Data Preparation",
    "section": "",
    "text": "# Import the packages we'll need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.offline as py\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nimport matplotlib.ticker as mtick\nfrom yellowbrick.cluster import KElbowVisualizer  \nfrom sklearn.linear_model import BayesianRidge, LinearRegression, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score    \nfrom sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = 999\n\nLoad the libraries required in this study, mainly data processing libraries, machine learning algorithm libraries, clustering algorithm libraries and interactive visualization libraries.",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/Introduction.html#load-data",
    "href": "analysis/Introduction.html#load-data",
    "title": "Data Preparation",
    "section": "Load Data",
    "text": "Load Data\n\n\nCode\nfrom itables import init_notebook_mode, show\ndf = pd.read_csv(\"real_estate_utah.csv\")\ninit_notebook_mode(all_interactive=False)\nshow(df)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      type\n      text\n      year_built\n      beds\n      baths\n      baths_full\n      baths_half\n      garage\n      lot_sqft\n      sqft\n      stories\n      lastSoldOn\n      listPrice\n      status\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nDemonstrate the overall value of the dataset",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/Introduction.html#feature-engineering",
    "href": "analysis/Introduction.html#feature-engineering",
    "title": "Data Preparation",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n\nCode\ndf['type'] = df['type'].map(type_mapping)\ndf['year_built'] = df['year_built'].astype('int')\ndf['age'] = pd.to_datetime('today').year - df['year_built']\ndf= df.loc[(df[\"sqft\"] &gt; 0.0) & (df['listPrice'] &gt; 0.0)]\ndf['pricePerSqft'] = df['listPrice'] / df['sqft'] #create feature\ndf.drop('year_built', axis=1, inplace=True)\ndf['lastSoldOn'] = pd.to_datetime(df['lastSoldOn'])\ndf = df.drop(['listPrice','sqft'],axis=1)",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/Introduction.html#missing-value-analysis",
    "href": "analysis/Introduction.html#missing-value-analysis",
    "title": "Data Preparation",
    "section": "Missing value analysis",
    "text": "Missing value analysis\n\n\nCode\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #Visual inspection of missing values\n\n\n\n\n\n\n\n\n\nChecking for missing data revealed no missing values for the variables in the dataset.\n\n\nCode\ntype_mapping = {\n    'single_family': 'single_family',\n    'land': 'land',\n    'townhomes': 'townhome',\n    'townhouse': 'townhome',\n    'mobile': 'mobile',\n    'condos': 'condo',\n    'condo_townhome_rowhome_coop': 'condo',\n    'condo_townhome': 'condo',\n    'condo': 'condo',\n    'other': 'other',\n    'farm': 'farm'\n}",
    "crumbs": [
      "Analysis",
      "Data Preparation"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#interactive-word-cloud-diagram",
    "href": "analysis/Advanced Data Visualization.html#interactive-word-cloud-diagram",
    "title": "Advanced Visualization",
    "section": "",
    "text": "Code\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\ntext = \" \".join(str(each) for each in df.text)\nwc = WordCloud(stopwords = set(STOPWORDS),\n               max_words = 200,\n               max_font_size = 100)\nwc.generate(text)\nword_list=[]\nfreq_list=[]\nfontsize_list=[]\nposition_list=[]\norientation_list=[]\ncolor_list=[]\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    word_list.append(word)\n    freq_list.append(freq)\n    fontsize_list.append(fontsize)\n    position_list.append(position)\n    orientation_list.append(orientation)\n    color_list.append(color)\n# get the positions\nx=[]\ny=[]\nfor i in position_list:\n    x.append(i[0])\n    y.append(i[1])\n# get the relative occurence frequencies\nnew_freq_list = []\nfor i in freq_list:\n    new_freq_list.append(i*100)\nnew_freq_list\ntrace = go.Scatter(x=x, \n                   y=y, \n                   textfont = dict(size=new_freq_list,\n                                   color=color_list),\n                   hoverinfo='text',\n                   hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],\n                   mode='text',  \n                   text=word_list\n                  )\nlayout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=700,  \n    height=700  \n    )\npy.iplot(fig)\n\n\n                                                \n\n\nFor text fields in the data, they are presented through an interactive word cloud map, which displays a large amount of text data, thus allowing the reader to quickly grasp the point.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#interactive-line-chart",
    "href": "analysis/Advanced Data Visualization.html#interactive-line-chart",
    "title": "Advanced Visualization",
    "section": "Interactive Line Chart",
    "text": "Interactive Line Chart\n\n\nCode\ndfdate = df.groupby(\"lastSoldOn\")[\"pricePerSqft\"].mean().reset_index()\nfig = go.Figure(data=[go.Scatter(x=dfdate['lastSoldOn'], y=dfdate['pricePerSqft'], mode='lines')])\nfig.update_layout(title='Interactive Line Chart', xaxis_title='Date', yaxis_title='Value')\nfig.show()\n\n\n                                                \n\n\nInteractive time series graphs are used to show the average daily house prices, and the graphs are interactive and easy for users to click and view directly. According to the results of the line graph, it can be seen that the house price has been in fluctuation.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#correlation-coefficient-graph",
    "href": "analysis/Advanced Data Visualization.html#correlation-coefficient-graph",
    "title": "Advanced Visualization",
    "section": "Correlation coefficient graph",
    "text": "Correlation coefficient graph\n\n\nCode\npy.init_notebook_mode(connected=True)\ndfc = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\"]]\ncorr = dfc.corr()\nmatrix_cols = corr.columns.tolist()\ncorr_array = np.array(corr)\ntrace = go.Heatmap(x=matrix_cols,\n                  y=matrix_cols,\n                  z=corr_array,\n                  colorscale=\"Viridis\",\n                  colorbar=dict())\nlayout = go.Layout(dict(title=\"Correlation Matrix for variables\"),\n                  margin=dict(r = 0 ,\n                              l = 100,\n                              t = 0,\n                              b = 100),\n                   yaxis=dict(tickfont=dict(size = 9)),\n                   xaxis=dict(tickfont=dict(size = 9)),\n                  )\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nAn interactive correlation coefficient plot shows the relationship between the number of bedrooms, the total number of bathrooms, the number of fully furnished bathrooms, the number of half bathrooms, the number of garage spaces, the size of the parcel and the size of the property, and the price of the house. A correlation coefficient greater than 0 indicates a positive correlation between the two variables and a correlation coefficient less than 0 indicates a negative correlation between the two variables.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#interactive-scatterplot",
    "href": "analysis/Advanced Data Visualization.html#interactive-scatterplot",
    "title": "Advanced Visualization",
    "section": "Interactive scatterplot",
    "text": "Interactive scatterplot\n\n\nCode\nfig = px.scatter(\n  df,x=\"pricePerSqft\" \n  ,y=\"lot_sqft\"  \n  ,color=\"type\"  \n  ,size_max=60 \n)\nfig.update_layout(\n    width=500,  #\n    height=500  \n)\npy.iplot(fig)\n\n\n                                                \n\n\nThe relationship between parcel size and house price per unit area is demonstrated through an interactive scatterplot, from which the scatterplot can be found to show a positive correlation between parcel size and house price per unit area.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#interactive-box-diagram",
    "href": "analysis/Advanced Data Visualization.html#interactive-box-diagram",
    "title": "Advanced Visualization",
    "section": "Interactive Box Diagram",
    "text": "Interactive Box Diagram\n\n\nCode\nfig3 = px.box(df, x=\"type\", y=\"pricePerSqft\")\nfig3.show()\n\n\n                                                \n\n\nThe distribution of house price per unit area for different types of land parcels is demonstrated by box plots, which show the maximum value, minimum value, median and other information, and this figure shows that there are differences in the distribution of house price per unit area for different types of land parcels.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Advanced Data Visualization.html#interactive-bar-chart",
    "href": "analysis/Advanced Data Visualization.html#interactive-bar-chart",
    "title": "Advanced Visualization",
    "section": "Interactive bar chart",
    "text": "Interactive bar chart\n\n\nCode\ndf1 = df[\"type\"].value_counts().reset_index()\ndf1.columns = [\"type\",\"Count\"]\nfig = px.bar(df1,\n             x=\"Count\",\n             text=\"Count\",\n             orientation=\"h\")\nfig.show()\n\n\n                                                \n\n\nDemonstrate the distribution of the number of different parcels through an interactive bar chart.",
    "crumbs": [
      "Analysis",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "analysis/Machine Learning.html#comprehensive-clustering-algorithm",
    "href": "analysis/Machine Learning.html#comprehensive-clustering-algorithm",
    "title": "Machine Learning Algorithms",
    "section": "",
    "text": "Five machine learning models were constructed to predict home prices in Utah: Bayesian Ridge, Linear Regression, ElasticNet, SVR, and GBR. These models are trained based on the characteristics of houses (e.g., size, number of bedrooms, etc.) to predict house prices. To evaluate the performance of the models, a cross-validation approach was used to ensure the stability and reliability of the results through multiple training and testing.\n\n\nCode\nX = df.drop(['text','pricePerSqft',\"lastSoldOn\"],axis=1)\ny = df['pricePerSqft']\nX = pd.get_dummies(X, drop_first=True, dtype='int')\n\n\n\n\nCode\nX = X\ny = df['pricePerSqft'].values\nn_folds=5\n#Bayesling regression model\nbr_model = BayesianRidge()\n#Linear regression\nlr_model = LinearRegression()\n#Elastic network regression model\netc_model = ElasticNet()\n#Support vector machine regression\nsvr_model = SVR()\n#Gradient enhanced regression model\ngbr_model = GradientBoostingRegressor(random_state=1)\n#A list of names for different models\nmodel_names = ['BayesianRidge', 'LinearRegression', 'ElasticNet', 'SVR', 'GBR']\nmodel_dic = [br_model, lr_model, etc_model, svr_model, gbr_model]\ncv_score_list = []\npre_y_list = []\nfor model in model_dic:\n    scores = cross_val_score(model,X,y,cv=n_folds) #Model cross-validation\n    cv_score_list.append(scores)                  \n    pre_y_list.append(model.fit(X,y).predict(X))   #Prediction results on the test set\ndf_score = pd.DataFrame(cv_score_list, index=model_names)\ndf_score\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nBayesianRidge\n0.331879\n0.476998\n-0.246165\n0.375415\n0.245122\n\n\nLinearRegression\n0.332096\n0.483609\n-0.253943\n0.378228\n0.245462\n\n\nElasticNet\n0.130439\n0.001475\n-0.010112\n0.047115\n0.087906\n\n\nSVR\n-0.026903\n-0.018592\n-0.007262\n-0.012425\n-0.021750\n\n\nGBR\n0.591863\n0.481195\n0.190811\n0.603570\n0.266486\n\n\n\n\n\n\n\n\n\nCode\n### Evaluation of model effect index ###\nn_sample, n_feature = X.shape\n# Regression evaluation index object list\nmodel_metrics_name = [explained_variance_score,\n                      mean_absolute_error,\n                      mean_squared_error,r2_score]\n# Regression evaluation index list\nmodel_metrics_list = []\n# Loop through the predictions of each model\nfor pre_y in pre_y_list:\n    tmp_list = []\n    for mdl in model_metrics_name:\n        tmp_score = mdl(y, pre_y)\n        tmp_list.append(tmp_score)\n    model_metrics_list.append(tmp_list)\ndf_met = pd.DataFrame(model_metrics_list, index=model_names,\n                      columns=['ev', 'mae', 'mse', 'r2'])\ndf_met\n\n\n\n\n\n\n\n\n\nev\nmae\nmse\nr2\n\n\n\n\nBayesianRidge\n0.310648\n116.273722\n99634.907167\n0.310648\n\n\nLinearRegression\n0.310672\n116.214159\n99631.443272\n0.310672\n\n\nElasticNet\n0.038671\n125.347216\n138944.822627\n0.038671\n\n\nSVR\n0.004330\n121.447129\n146445.779432\n-0.013226\n\n\nGBR\n0.728965\n88.184705\n39173.747357\n0.728965\n\n\n\n\n\n\n\nBased on the evaluation metrics provided, the gradient boosting regression model demonstrated optimal performance in predicting Utah home prices. The mean absolute error and mean square error were better than other models, and the R-squared value was as high as 0.728965, indicating that the model explains a large portion of the home price variability. Despite the relatively low mae of ElasticNet, GBR is superior in terms of the composite metrics, especially its mse is much lower than that of the other models, showing higher prediction accuracy. Thus, the gradient boosted regression model is considered the optimal choice for predicting house prices in Utah.\n\n\nCode\nplt.figure(figsize=(9, 7))\ncolor_list = ['r','g','b','y','c']\nfor i, pre_y in enumerate(pre_y_list):\n    plt.subplot(2, 3, i+1)\n    plt.plot(np.arange(X.shape[0]), y, color='k', label='y')\n    # Draw prediction lines for each model\n    plt.plot(np.arange(X.shape[0]),\n             pre_y,\n             color_list[i],\n             label=model_names[i])\n    plt.title(model_names[i])\n    plt.legend(loc='lower left')\nplt.show()\n\n\n\n\n\n\n\n\n\nBy comparing the results of multiple regression models, we can see that the best model is the gradient enhanced regression model.",
    "crumbs": [
      "Analysis",
      "Machine Learning Algorithms"
    ]
  },
  {
    "objectID": "analysis/Machine Learning.html#optimal-algorithmic-grid-tuning-parameters",
    "href": "analysis/Machine Learning.html#optimal-algorithmic-grid-tuning-parameters",
    "title": "Machine Learning Algorithms",
    "section": "Optimal Algorithmic Grid Tuning Parameters",
    "text": "Optimal Algorithmic Grid Tuning Parameters\n\n\nCode\ndef MSE(y_true, y_pred):\n    return ((y_true - y_pred) ** 2).mean()\n\n\n\n\nCode\nX = df.drop(['text','pricePerSqft',\"lastSoldOn\"],axis=1)\ny = df['pricePerSqft']\nX = pd.get_dummies(X, drop_first=True, dtype='int')\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2,test_size=0.2)\n#Grid Search\nparams={'subsample':[0.65, 0.7, 0.75],\n        'n_estimators':[30, 50, 100],\n        'learning_rate':[0.05, 0.075, 0.1],\n        'max_depth':[3, 4, 5]\n        }\ngbr = GradientBoostingRegressor(random_state=1)\n# Instantiate RandomizedSearchCV as rand_reg\nrand_reg = RandomizedSearchCV(gbr, params, n_iter=3, scoring='neg_mean_squared_error', \n                              cv=5, n_jobs=-1, random_state=2)\nrand_reg.fit(X_train, y_train)\nbest_model = rand_reg.best_estimator_\nbest_params = rand_reg.best_params_\nprint(\"Best params:\", best_params)\nbest_score = np.sqrt(-rand_reg.best_score_)\nprint(\"Training score: {:.3f}\".format(best_score))\ny_pred = best_model.predict(X_test)\nrmse_test = MSE(y_test, y_pred)**0.5\nprint('Test set score: {:.3f}'.format(rmse_test))\n\n\nBest params: {'subsample': 0.65, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.075}\nTraining score: 287.294\nTest set score: 396.865\n\n\nThe RandomizedSearchCV method was used to randomly search for the optimal combinations within the specified parameter ranges by performing mesh tuning on the gradient-enhanced model. Eventually, the optimal model parameters were obtained and the model was trained based on these parameters. The performance of the model on the training and test sets was evaluated by the root mean square error, and the results showed the effectiveness and prediction accuracy of the model.",
    "crumbs": [
      "Analysis",
      "Machine Learning Algorithms"
    ]
  },
  {
    "objectID": "analysis/Machine Learning.html#model-variable-importance-results",
    "href": "analysis/Machine Learning.html#model-variable-importance-results",
    "title": "Machine Learning Algorithms",
    "section": "Model Variable Importance Results",
    "text": "Model Variable Importance Results\n\n\nCode\n# Importance of variable\npd.concat((pd.DataFrame(X.columns,columns = ['feature']), \n           pd.DataFrame(rand_reg.best_estimator_.feature_importances_,\n                        columns = ['importance'])), \n          axis = 1).sort_values(by='importance',\n                                ascending = False)\n\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n5\nlot_sqft\n0.667378\n\n\n1\nbaths\n0.111691\n\n\n4\ngarage\n0.069299\n\n\n3\nbaths_half\n0.067300\n\n\n2\nbaths_full\n0.033914\n\n\n7\nage\n0.020846\n\n\n9\ntype_land\n0.011464\n\n\n0\nbeds\n0.009646\n\n\n10\ntype_mobile\n0.003871\n\n\n8\ntype_farm\n0.002482\n\n\n6\nstories\n0.002109\n\n\n11\ntype_other\n0.000000\n\n\n12\ntype_single_family\n0.000000\n\n\n13\ntype_townhome\n0.000000\n\n\n14\nstatus_ready_to_build\n0.000000\n\n\n\n\n\n\n\nThe results of the optimal gradient boosting regression model, and the variable importance results show that parcel size has the greatest impact on house price prediction with 66.7%. baths (number of baths) follows with 11.2%.Other variables such as garage and baths_half also have an impact, while characteristics such as housing type and condition have little effect on the predictions.\n\n\nCode\n# Plot feature importance\nfeature_importance = rand_reg.best_estimator_.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 1, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()",
    "crumbs": [
      "Analysis",
      "Machine Learning Algorithms"
    ]
  },
  {
    "objectID": "analysis/Clustering Algorithms.html#selecting-k-with-the-elbow-method",
    "href": "analysis/Clustering Algorithms.html#selecting-k-with-the-elbow-method",
    "title": "Cluster analysis",
    "section": "",
    "text": "Code\ndf[\"lot_sqft\"] = (df[\"lot_sqft\"] - df[\"lot_sqft\"].min()) / (df[\"lot_sqft\"].max() - df[\"lot_sqft\"].min())\ndfclu = df.loc[:,[\"beds\",\"baths\",\"baths_full\",\"baths_half\",\"garage\",\"lot_sqft\",\"pricePerSqft\",\"stories\",\"age\"]]\n\n\n\n\nCode\nK = range(1, 10)\nmeandistortions = []\nfor k in K:\n    kmeans = KMeans(n_clusters=k,random_state=1)\n    kmeans.fit(dfclu)\n    meandistortions.append(sum(np.min(cdist(dfclu , kmeans.cluster_centers_, 'euclidean'), axis=1))/df1.shape[0])\nplt.plot(K, meandistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average Dispersion')\nplt.title('Selecting k with the Elbow Method')\nplt.gca().yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe optimal number of clusters for the K-means clustering algorithm is selected by the elbow method, k. First, a range of cluster numbers from 1 to 9 is defined, and then for each value of k, the mean aberration after clustering, i.e., the average distance of a sample to its nearest cluster center, is calculated. Finally, the k values are plotted against the mean aberration to aid in the selection of the optimal k value.\n\n\nCode\nmodel = KMeans(random_state=1)  \nvisualizer = KElbowVisualizer(\n    model, \n    k=(2,10),\n    timings=True,  # \n    metric=\"distortion\",  # distortion, silhouette,calinski_harabasz\n    )\nvisualizer.fit(dfclu)        \nvisualizer.show()  \n\n\n\n\n\n\n\n\n\nThe KMeans clustering model and the KElbowVisualizer visualization tool are used to select the optimal number of clusters, and the KElbowVisualizer helps the user find the “elbow” intuitively by plotting the distortion at different k-values, i.e., the point of k-value where the distortion starts to decrease sharply and becomes slower, and the optimal number of clusters is obtained. values, and get the optimal number of clusters4.",
    "crumbs": [
      "Analysis",
      "Cluster analysis"
    ]
  },
  {
    "objectID": "analysis/Clustering Algorithms.html#kmeans-clustering-algorithm",
    "href": "analysis/Clustering Algorithms.html#kmeans-clustering-algorithm",
    "title": "Cluster analysis",
    "section": "Kmeans clustering algorithm",
    "text": "Kmeans clustering algorithm\n\n\nCode\nfrom sklearn import metrics\nkmeans = KMeans(n_clusters=4, random_state=1)\nkmeans.fit(dfclu)\n\n\nKMeans(n_clusters=4, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=4, random_state=1) \n\n\nBased on the best clustering result, mean clustering is performed.",
    "crumbs": [
      "Analysis",
      "Cluster analysis"
    ]
  },
  {
    "objectID": "analysis/Clustering Algorithms.html#evaluation-of-clustering-algorithms",
    "href": "analysis/Clustering Algorithms.html#evaluation-of-clustering-algorithms",
    "title": "Cluster analysis",
    "section": "Evaluation of clustering algorithms",
    "text": "Evaluation of clustering algorithms\n\n\nCode\npre_y = kmeans.predict(dfclu)\ninertias = kmeans.inertia_\ny = df['status']\n# Adjusted Rand Index\nadjusted_rand_s = metrics.adjusted_rand_score(y, pre_y)\n# Mutual information\nmutual_info_s = metrics.mutual_info_score(y, pre_y)\n# Adjusted mutual information\nadjusted_mutual_info_s = metrics.adjusted_mutual_info_score(y, pre_y)\n# Homogenization score\nhomogeneity_s = metrics.homogeneity_score(y, pre_y)\n# Integrity score\ncompleteness_s = metrics.completeness_score(y, pre_y)\n# V-measure score\nv_measure_s = metrics.v_measure_score(y, pre_y)\n# Average profile factor\nsilhouette_s = metrics.silhouette_score(dfclu, pre_y, metric='euclidean')\n# Calinski and Harabaz scores\ncalinski_harabaz_s = metrics.calinski_harabasz_score(dfclu, pre_y)\ndf_metrics = pd.DataFrame([[inertias, adjusted_rand_s,mutual_info_s, adjusted_mutual_info_s, homogeneity_s,completeness_s,\n                            v_measure_s, silhouette_s ,calinski_harabaz_s]],\n                             columns=['ine','tARI','tMI','tAMI','thomo','tcomp','tv_m','tsilh','tc&h'])\n    \ndf_metrics\n\n\n\n\n\n\n\n\n\nine\ntARI\ntMI\ntAMI\nthomo\ntcomp\ntv_m\ntsilh\ntc&h\n\n\n\n\n0\n7.342729e+07\n-0.052896\n0.002992\n0.010274\n0.013685\n0.010176\n0.011672\n0.721808\n11390.754282\n\n\n\n\n\n\n\nThe clustering results show the values of several evaluation metrics. These include lower values for the adjusted Rand index, mutual information, adjusted mutual information, etc., and higher values for the adjusted Rand index.",
    "crumbs": [
      "Analysis",
      "Cluster analysis"
    ]
  },
  {
    "objectID": "index.html#welcome-to-our-final-project",
    "href": "index.html#welcome-to-our-final-project",
    "title": "Research on listing price prediction of real estate based on machine learning model",
    "section": "",
    "text": "We are interested in home price prediction and have selected a Utah real estate dataset for our project analysis.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-the-project",
    "href": "index.html#about-the-project",
    "title": "Research on listing price prediction of real estate based on machine learning model",
    "section": "About the Project",
    "text": "About the Project\nThis project uses a Utah real estate dataset to build machine learning models to predict real estate listing prices. The dataset contains 4,440 real estate entries, each covering 14 different attributes, providing a comprehensive and detailed description of the various characteristics and conditions of a property. Analysis Goal: To accurately predict the listing price of a property through a variety of visualization and clustering algorithms, machine learning regression models, and other methods. To achieve this goal, several key attributes of the property will be considered, including property type, detailed description, year built, number of bedrooms and bathrooms, amount of garage space, lot size, total house square footage, floor layout, list price, and last sale date. Analyze and understand in depth the extent to which these factors affect the price of the property.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#answer-the-following-questions-primarily",
    "href": "index.html#answer-the-following-questions-primarily",
    "title": "Research on listing price prediction of real estate based on machine learning model",
    "section": "Answer the following questions primarily",
    "text": "Answer the following questions primarily\nWhat are the relationships between the variables in the data set?\nHow do different attributes affect the listing price? This helps to identify the key factors that determine the value of the property.\nWhich is the most accurate model for predicting house prices?\nThe research in this project contributes to a better understanding of the real estate market and provides a valuable reference for real estate valuation and investment decisions.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Research on listing price prediction of real estate based on machine learning model",
    "section": "Data source",
    "text": "Data source\nhttps://www.kaggle.com/datasets/kanchana1990/real-estate-data-utah-2024",
    "crumbs": [
      "Welcome"
    ]
  }
]